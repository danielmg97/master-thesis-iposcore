# -*- coding: utf-8 -*-
"""OPT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qWTTb65nJA4vloy4nBS8510EVF3dnrVz
"""

import math
import numpy as np
import pandas as pd
import global_variables
import load_and_run

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn import linear_model
from sklearn import svm
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import PolynomialFeatures
from xgboost import XGBRegressor
from sklearn.cross_decomposition import PLSRegression
from sklearn.neural_network import MLPRegressor

from hyperopt import STATUS_OK
from hyperopt import tpe
from hyperopt import Trials
from hyperopt import fmin
from hyperopt import hp

MAX_EVALS = 100
OUTPUT = global_variables.outputs[2]            # CHANGE ACCORDINGLY

variables = global_variables.binarias+global_variables.categoricas+global_variables.numericas
# variables = ['ARISCAT procedimento emergente', 'ARISCAT anemia pré-operativa', 'morte (%)', 'pneumonia (%)', 'complicações sérias (%)', 'ACS - previsão dias internamento', 'qualquer complicação (%)', 'Discharge to Nursing or Rehab Facility (%)', '% mortalidade P-Possum', 'falência renal (%)', 'complicações cardíacas (%)', 'reoperação (%)', '% morbilidade P-Possum', 'tromboembolismo venoso (%)', 'Score gravidade cirúrgica P-Possum', 'Score fisiológico P-Possum', 'readmissão (%)', 'risco médio.11', 'ITU (%)', 'ARISCAT PONTUAÇÃO TOTAL', 'risco médio.12', 'risco médio', 'risco médio.1', 'risco médio.13', 'infeção cirúrgica (%)', 'risco médio.4', 'SCORE ARISCAT', 'risco médio.7']

dataset = load_and_run.load_data(OUTPUT,variables)

dataset.dropna(how="any",subset=dataset.columns[[-1]], inplace=True) #this is a bug fix

headers = dataset.columns
to_dummify = []
for i in range(0,len(headers)):
  if headers[i] in global_variables.categoricas:
    to_dummify.append(i)

dataset = dataset.to_numpy()

#Linear Regression
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    res = load_and_run.reg_k_fold(LinearRegression(**params),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'fit_intercept': hp.choice('fit_intercept', [True,False]),
'normalize': hp.choice('normalize', [True,False])}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   RIDGE
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    res = load_and_run.reg_k_fold(Ridge(**params),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'alpha': hp.loguniform('alpha', np.log(1.0), np.log(10.0)),
'fit_intercept': hp.choice('fit_intercept', [True,False]),
'normalize': hp.choice('normalize', [True,False]),
'solver': hp.choice('solver', ['auto'])}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   LASSO
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    res = load_and_run.reg_k_fold(linear_model.Lasso(**params),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'alpha': hp.loguniform('alpha', np.log(1.0), np.log(10.0)),
'fit_intercept': hp.choice('fit_intercept', [True,False]),
'precompute': hp.choice('precompute', [True,False]),

'normalize': hp.choice('normalize', [True,False])}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   SVM
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    res = load_and_run.reg_k_fold(svm.SVR(**params,cache_size=7000),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),
         'degree': hp.quniform('degree', 3, 5, 1),
         'gamma': hp.choice('gamma', ['scale','auto']),
         'C': hp.uniform('C', 0.0, 2.0),
         'shrinking': hp.choice('shrinking', [True,False])}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   ELASTICNET
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    res = load_and_run.reg_k_fold(ElasticNet(**params,random_state=0),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'alpha': hp.loguniform('alpha', np.log(1.0), np.log(10.0)),
         'fit_intercept': hp.choice('fit_intercept', [True,False]),
         'normalize': hp.choice('normalize', [True,False]),
         'l1_ratio': hp.uniform('l1_ratio', 0.0, 1.0)}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   KNN
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    params['n_neighbors']=int(params['n_neighbors'])
    res = load_and_run.reg_k_fold(KNeighborsRegressor(**params),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'n_neighbors': hp.quniform('n_neighbors', 2,50,2),
         'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),
         'weights': hp.choice('weights', ['distance', 'uniform'])}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   DECISION TREES
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    params['max_depth']=int(params['max_depth'])
    params['min_samples_split']=int(params['min_samples_split'])
    params['min_samples_leaf']=int(params['min_samples_leaf'])
    # params['max_features']=int(params['max_features'])
    res = load_and_run.reg_k_fold(DecisionTreeRegressor(**params,random_state=0),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'criterion': hp.choice('criterion', ['mse', 'friedman_mse', 'mae']),
'splitter': hp.choice('splitter', ['best','random']),
'max_depth': hp.quniform('max_depth', 10, 100, 5),
'min_samples_split': hp.quniform('min_samples_split', 2, 20, 2),
'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 20, 2),
'min_weight_fraction_leaf': hp.loguniform('min_weight_fraction_leaf', np.log(0.0001), np.log(0.5)),
# 'max_features': hp.quniform('max_features', 10, 100, 10),
'ccp_alpha': hp.loguniform('ccp_alpha', np.log(0.0001), np.log(0.2))}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   RANDOM FOREST
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    params['n_estimators']=int(params['n_estimators'])
    params['min_samples_split']=int(params['min_samples_split'])
    params['min_samples_leaf']=int(params['min_samples_leaf'])
    # params['max_features']=int(params['max_features'])
    res = load_and_run.reg_k_fold(RandomForestRegressor(**params, random_state=0),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'n_estimators': hp.quniform('n_estimators', 20, 200, 20),
'criterion': hp.choice('criterion', ['mse', 'mae']),
# 'max_depth': hp.quniform('max_depth', 1, 100, 5),
'min_samples_split': hp.quniform('min_samples_split', 2, 20, 2),
'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 20, 2),
'min_weight_fraction_leaf': hp.loguniform('min_weight_fraction_leaf', np.log(0.0001), np.log(0.5)),
# 'max_features': hp.quniform('max_features', 10, 100, 10),
#bootstrap
'ccp_alpha': hp.loguniform('ccp_alpha', np.log(0.0001), np.log(0.2))}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   XGBoost
tpe_algorithm = tpe.suggest
bayes_trials = Trials()
from xgboost import XGBRegressor
def objective(params):
    params['n_estimators']=int(params['n_estimators'])
    params['max_depth']=int(params['max_depth'])
    res = load_and_run.reg_k_fold(XGBRegressor(**params,objective='reg:squarederror'),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'max_depth': hp.quniform('max_depth', 3, 45, 3),
'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.9)),
'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart']),
'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(0.1)),
'gamma': hp.loguniform('gamma', np.log(0.0001), np.log(5.0)),
'n_estimators': hp.quniform('n_estimators', 20, 500, 20)}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

#   PLS
tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    params['n_components']=int(params['n_components'])
    res = load_and_run.reg_k_fold(PLSRegression(**params),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'n_components': hp.quniform('n_components', 1, 20, 1)}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))

tpe_algorithm = tpe.suggest
bayes_trials = Trials()

def objective(params):
    params['batch_size']=int(params['batch_size'])
    res = load_and_run.reg_k_fold(MLPRegressor(**params,max_iter=2000),dataset,headers,to_dummify)
    return {'loss': res, 'params': params, 'status': STATUS_OK}

space = {'activation': hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),
'solver': hp.choice('solver', ['lbfgs', 'sgd', 'adam']),
'alpha': hp.loguniform('alpha', np.log(0.0001), np.log(0.2)),
'learning_rate_init': hp.loguniform('learning_rate_init', np.log(0.001), np.log(0.2)),
'batch_size': hp.quniform('batch_size', 50, 300, 50),
'early_stopping': hp.choice('early_stopping', [True,False]),
'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),
'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(50,50,50,), (50,100,50,), (100,100,),(50,50,),(50,25,),(50,),(25)])}

print(fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials))